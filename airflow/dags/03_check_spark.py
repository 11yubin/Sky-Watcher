# 03_check_spark.py
# spark ì‹¤í–‰ ì—¬ë¶€ë¥¼ 10ë¶„ë§ˆë‹¤ ì²´í¬í•˜ê³ , ì•ˆë˜ë©´ ë‹¤ì‹œ ì‹¤í–‰ -> ì»¨í…Œì´ë„ˆë¥¼ ê°ì‹œí•˜ëŠ” dag
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import docker

def check_and_restart_spark():
    client = docker.from_env()
    
    target_container = 'skywatcher-spark'
    
    try:
        container = client.containers.get(target_container)
        print(f"ğŸ‘€ ìƒíƒœ í™•ì¸: {target_container} is {container.status}")
        
        if container.status != 'running':
            print("ğŸš¨ ìŠ¤íŒŒí¬ ì •ì§€ ìƒíƒœ! ì¬ì‹œì‘ í”„ë¡œì„¸ìŠ¤ ê°€ë™ âš¡ï¸")
            container.restart()
            print("âœ… ì¬ì‹œì‘ ì™„ë£Œ.")
        else:
            print("âœ… ìŠ¤íŒŒí¬ëŠ” íŠ¼íŠ¼í•©ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ: {e}")

with DAG(
    dag_id='skywatcher_health_check',
    start_date=datetime(2023, 1, 1),
    schedule_interval='*/10 * * * *', # 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    tags=['monitoring']
) as dag:

    check_task = PythonOperator(
        task_id='check_spark_container',
        python_callable=check_and_restart_spark
    )