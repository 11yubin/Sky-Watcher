version: '3.8'

services:
  postgres:
    image: postgres:15
    container_name: skywatcher-postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_DB: skywatcher
    ports:
      - "5434:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: skywatcher-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: skywatcher-kafka
    volumes:
      - ./kafka_data:/kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # 내부망(PLAINTEXT)과 외부망(CONNECTIONS_FROM_HOST)을 철저히 분리
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,CONNECTIONS_FROM_HOST://kafka:9092
      # 브로커가 컨트롤러와 통신할 때 내부망 주소(kafka)를 쓰도록 강제
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # 토픽 자동 생성 옵션
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: true

  spark:
    image: apache/spark:3.5.0
    container_name: skywatcher-spark
    depends_on:
      - producer
    environment:
      - SPARK_MODE=master
    user: root
    # tty: true
    # jdbc 연결 코드 추가
    command: >
      /opt/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0
      --master local[*]
      /opt/spark/work-dir/streaming/main.py
    volumes:
      - ./streaming:/opt/spark/work-dir/streaming

  grafana:
    image: grafana/grafana:latest
    container_name: skywatcher-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - postgres
    # 대시보드 생성 자동화 (프로비저닝 파일 경로 연결)
    volumes:
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      # db연결
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources

  # Airflow 컨테이너들
  # airflow 전용 db
  airflow-postgres:
    image: postgres:13
    ports:
      - "5433:5432"    
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow

  # airflow 대시보드 화면
  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-postgres
    ports:
      - "8080:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Sky --lastname Watcher --role Admin --email admin@example.com || true &&
      airflow webserver"
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False    # 예제 dag 안보이게
      - _PIP_ADDITIONAL_REQUIREMENTS=psycopg2-binary pandas requests
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  # airflow 스케줄러
  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    restart: always
    # user: root
    depends_on:
      - airflow-postgres
    command: scheduler
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _PIP_ADDITIONAL_REQUIREMENTS=psycopg2-binary pandas requests docker
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock

  # kafka producer
  producer:
    build:
      context: .
      dockerfile: producer/Dockerfile
    container_name: skywatcher-producer
    restart: on-failure       # Kafka가 아직 안 켜졌으면 죽었다가 다시 살아나게 함
    depends_on:
      - kafka
    volumes:
      - ./producer:/app/producer
      - ./pyproject.toml:/app/pyproject.toml
    env_file:
      - .env